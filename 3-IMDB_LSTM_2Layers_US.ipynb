{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1459395b",
   "metadata": {},
   "source": [
    "# LSTM Model for Movie Genre Prediction\n",
    "\n",
    "## Description:\n",
    "\n",
    "This notebook demonstrates the implementation of a Long Short-Term Memory (LSTM) neural network model for predicting movie genres. The primary goal is to develop a deep learning architecture capable of accurately classifying movie genres based on textual data such as movie titles, plot summaries, and sentiment analysis.\n",
    "\n",
    "In this notebook, we'll begin by preprocessing the textual data, which involves tokenization, lemmatization, and encoding of movie genres. We'll then split the dataset into training and testing sets to train and evaluate the LSTM model. The model architecture consists of two LSTM layers followed by a dense output layer with sigmoid activation to predict multiple genres simultaneously.\n",
    "\n",
    "Furthermore, we'll incorporate additional features such as sentiment analysis scores and dominant topics extracted from the text to enhance the predictive performance of the model. The training process involves optimizing the model's parameters using the binary cross-entropy loss function and the Adam optimizer.\n",
    "\n",
    "\n",
    "**Author:** [Caique Matos]\n",
    "**Date:** [04/16/24]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662e27cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\caiqu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\caiqu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d372e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd56b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/caiqu/OneDrive/Competitions/sony/input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f55396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(path+'df_train_under.csv')\n",
    "df_valid = pd.read_csv(path+\"df_validation_under.csv\")\n",
    "df_test = pd.read_csv(path+'df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f370b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'SRC_TITLE_ID', 'SRC_TITLE_NM', 'TITLE_TYPE', 'TITLE_YR',\n",
       "       'RELEASE_DT', 'RUN_TIME', 'PLOT_OUTLINE', 'PLOT_MEDIUM', 'PLOT_SUMMARY',\n",
       "       'RATING_AVG', 'NO_OF_VOTES', 'BUDGET_AMT', 'SRC_GENRE', 'RELEASE_DAY',\n",
       "       'RELEASE_MONTH', 'PLOT_OUTLINE_SENTIMENT', 'PLOT_MEDIUM_SENTIMENT',\n",
       "       'PLOT_SUMMARY_SENTIMENT', 'SRC_TITLE_NM_SENTIMENT',\n",
       "       'PLOT_SUMMARY_SENTIMENT_ENCODED', 'SRC_TITLE_NM_SENTIMENT_ENCODED',\n",
       "       'PLOT_OUTLINE_SENTIMENT_ENCODED', 'PLOT_MEDIUM_SENTIMENT_ENCODED',\n",
       "       'PLOT_SUMMARY_DOMINANT_TOPIC', 'PLOT_OUTLINE_DOMINANT_TOPIC',\n",
       "       'PLOT_MEDIUM_DOMINANT_TOPIC', 'SRC_TITLE_NM_DOMINANT_TOPIC',\n",
       "       'SRC_GENRE_ENCODED'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a54fb9",
   "metadata": {},
   "source": [
    "## Pre-process and Extracting Value from text columns\n",
    "\n",
    "\n",
    "A abordagem escolhida foi agrupar todas as colunas de texto(título e colunas de resumo de enredo), passando por uma tokenização e por uma fase de \"lematização\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6afb9ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-processamento do texto\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token.isalnum()]\n",
    "        return ' '.join(filtered_tokens)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "df_train['processed_title'] = df_train['SRC_TITLE_NM'].fillna('').apply(preprocess_text)\n",
    "df_train['processed_plot'] = df_train['PLOT_SUMMARY'].fillna('').apply(preprocess_text)\n",
    "df_train['processed_plot_outline'] = df_train['PLOT_OUTLINE'].fillna('').apply(preprocess_text)\n",
    "df_train['processed_plot_medium'] = df_train['PLOT_MEDIUM'].fillna('').apply(preprocess_text)\n",
    "df_train['combined_text'] = df_train['processed_title'] + ' ' + df_train['processed_plot'] + df_train['processed_plot_outline']+df_train['processed_plot_medium'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86771969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-processamento dos gêneros\n",
    "df_train['SRC_GENRE'] = df_train['SRC_GENRE'].apply(lambda x: x.split('|'))\n",
    "mlb = MultiLabelBinarizer()\n",
    "genres_encoded = mlb.fit_transform(df_train['SRC_GENRE'])\n",
    "\n",
    "# Carregar os embeddings GloVe pré-treinados\n",
    "embeddings_index = {}\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d705213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar e preencher as sequências de texto\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_train['combined_text'])\n",
    "sequences = tokenizer.texts_to_sequences(df_train['combined_text'])\n",
    "word_index = tokenizer.word_index\n",
    "padded_sequences = pad_sequences(sequences, maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "221f21b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar a matriz de embedding\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12cf834",
   "metadata": {},
   "source": [
    "## LSTM Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426131b8",
   "metadata": {},
   "source": [
    "1. Inputs:\n",
    "\n",
    "- Two input layers are defined:\n",
    "    - input_text: Represents the textual data with a shape of (300,), indicating a sequence length of 300.\n",
    "    - input_sentiment: Represents the sentiment and dominant topics features with a shape of (9,), indicating 9 sentiment features.\n",
    "\n",
    "\n",
    "2. Word Embedding Layer:\n",
    "\n",
    "    - Utilizes the Embedding layer to convert textual input into dense vectors of fixed size.\n",
    "    - Parameters:\n",
    "        - len(word_index) + 1: Specifies the size of the vocabulary.\n",
    "        - 100: Specifies the dimensionality of the embedding vector.\n",
    "        - weights=[embedding_matrix]: Uses pre-trained word embeddings.\n",
    "        - input_length=300: Specifies the length of input sequences.\n",
    "        - trainable=False: Freezes the embedding layer during training to retain pre-trained weights.\n",
    "\n",
    "\n",
    "3. LSTM Layer:\n",
    "\n",
    "- Utilizes the LSTM layer for sequence modeling and feature extraction.\n",
    "    - Parameters:\n",
    "        - 128: Specifies the dimensionality of the output space.\n",
    "        - dropout=0.2: Applies dropout to the input units.\n",
    "        - recurrent_dropout=0.2: Applies dropout to the recurrent units.\n",
    "\n",
    "4. Concatenation:\n",
    "\n",
    "- Concatenates the output of the LSTM layer with the input sentiment features.\n",
    "\n",
    "\n",
    "5. Output Layer:\n",
    "\n",
    "- Utilizes the Dense layer with a sigmoid activation function.\n",
    "- Outputs a binary classification probability for each genre class.\n",
    "\n",
    "**No focused work has been developed to optimize parameters such as batch size and epochs yet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ea8618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "128/128 [==============================] - 109s 835ms/step - loss: 0.3636 - accuracy: 0.3772 - val_loss: 0.3237 - val_accuracy: 0.4463\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 107s 838ms/step - loss: 0.3032 - accuracy: 0.5010 - val_loss: 0.3004 - val_accuracy: 0.5151\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 108s 845ms/step - loss: 0.2823 - accuracy: 0.5456 - val_loss: 0.2849 - val_accuracy: 0.5513\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 113s 880ms/step - loss: 0.2725 - accuracy: 0.5528 - val_loss: 0.2737 - val_accuracy: 0.5630\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 116s 903ms/step - loss: 0.2569 - accuracy: 0.5933 - val_loss: 0.2688 - val_accuracy: 0.5693\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 115s 900ms/step - loss: 0.2483 - accuracy: 0.6047 - val_loss: 0.2642 - val_accuracy: 0.5708\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 116s 903ms/step - loss: 0.2420 - accuracy: 0.6180 - val_loss: 0.2629 - val_accuracy: 0.5820\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 115s 903ms/step - loss: 0.2340 - accuracy: 0.6298 - val_loss: 0.2639 - val_accuracy: 0.5771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a20670ddf0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define inputs\n",
    "input_text = Input(shape=(300,))\n",
    "input_sentiment = Input(shape=(9,))\n",
    "\n",
    "# Word embedding layer\n",
    "embedding_text = Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=300, trainable=False)(input_text)\n",
    "\n",
    "# LSTM layer\n",
    "lstm_text = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(embedding_text)\n",
    "\n",
    "# Concatenate LSTM output with sentiment input\n",
    "concatenated = Concatenate()([lstm_text, input_sentiment])\n",
    "\n",
    "# Output layer\n",
    "output = Dense(len(mlb.classes_), activation='sigmoid')(concatenated)\n",
    "\n",
    "# Define model\n",
    "model = Model(inputs=[input_text, input_sentiment], outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(padded_sequences, genres_encoded, test_size=0.2, random_state=42)\n",
    "X_train_sentiment, X_test_sentiment, _, _ = train_test_split(df_train[['PLOT_SUMMARY_SENTIMENT_ENCODED', 'SRC_TITLE_NM_SENTIMENT_ENCODED',\n",
    "       'PLOT_OUTLINE_SENTIMENT_ENCODED', 'PLOT_MEDIUM_SENTIMENT_ENCODED',\n",
    "       'PLOT_SUMMARY_DOMINANT_TOPIC', 'PLOT_OUTLINE_DOMINANT_TOPIC',\n",
    "       'PLOT_MEDIUM_DOMINANT_TOPIC', 'SRC_TITLE_NM_DOMINANT_TOPIC','RATING_AVG']], genres_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model.fit([X_train_text, X_train_sentiment], y_train, batch_size=64, epochs=8, validation_data=([X_test_text, X_test_sentiment], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6091e9",
   "metadata": {},
   "source": [
    "### Exploring Training/Validating Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f55ce85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 8s 115ms/step\n",
      "Overall Accuracy: 0.5771484375\n",
      "Genre-wise Accuracy:\n",
      "\n",
      "Action: 0.90673828125\n",
      "Adventure: 0.974609375\n",
      "Comedy: 0.8310546875\n",
      "Crime: 0.95849609375\n",
      "Documentary: 0.88916015625\n",
      "Drama: 0.75\n",
      "Horror: 0.935546875\n"
     ]
    }
   ],
   "source": [
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_probs = model.predict([X_test_text, X_test_sentiment])\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "predicted_genres = mlb.classes_[y_pred_classes]\n",
    "\n",
    "# Calcular a acurácia média\n",
    "accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred_classes)\n",
    "\n",
    "# Calcular a acurácia por gênero\n",
    "genre_accuracy = {}\n",
    "for i, genre in enumerate(mlb.classes_):\n",
    "    genre_accuracy[genre] = accuracy_score(y_test[:, i], y_pred_probs[:, i] > 0.5)\n",
    "\n",
    "# Criar DataFrame com as previsões e os valores reais\n",
    "df_predictions = pd.DataFrame({\n",
    "    'Real_Genre': mlb.inverse_transform(y_test),\n",
    "    'Predicted_Genre': predicted_genres\n",
    "})\n",
    "\n",
    "# Exibir resultados\n",
    "print('Overall Accuracy:', accuracy)\n",
    "print('Genre-wise Accuracy:\\n')\n",
    "for genre, acc in genre_accuracy.items():\n",
    "    print(f'{genre}: {acc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb7b618",
   "metadata": {},
   "source": [
    "### Outsample Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d1c4c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 2s 105ms/step\n",
      "Overall Accuracy (Validation): 0.5743494423791822\n",
      "\n",
      "\n",
      "Genre-wise Accuracy (Validation):\n",
      "\n",
      "Action: 0.9070631970260223\n",
      "Adventure: 0.966542750929368\n",
      "Comedy: 0.8252788104089219\n",
      "Crime: 0.9553903345724907\n",
      "Documentary: 0.8773234200743495\n",
      "Drama: 0.7453531598513011\n",
      "Horror: 0.9479553903345725\n"
     ]
    }
   ],
   "source": [
    "# Pré-processamento do texto para os dados de validação\n",
    "\n",
    "\n",
    "df_valid['processed_title'] = df_valid['SRC_TITLE_NM'].fillna('').apply(preprocess_text)\n",
    "df_valid['processed_plot'] = df_valid['PLOT_SUMMARY'].fillna('').apply(preprocess_text)\n",
    "df_valid['processed_plot_outline'] = df_valid['PLOT_OUTLINE'].fillna('').apply(preprocess_text)\n",
    "df_valid['processed_plot_medium'] = df_valid['PLOT_MEDIUM'].fillna('').apply(preprocess_text)\n",
    "\n",
    "\n",
    "df_valid['combined_text'] = df_valid['processed_title'] + ' ' + df_valid['processed_plot'] + df_valid['processed_plot_outline']+df_valid['processed_plot_medium'] \n",
    "\n",
    "\n",
    "# Tokenizar e preencher as sequências de texto para os dados de validação\n",
    "sequences_valid = tokenizer.texts_to_sequences(df_valid['combined_text'])\n",
    "padded_sequences_valid = pad_sequences(sequences_valid, maxlen=300)\n",
    "\n",
    "\n",
    "# Pré-processamento dos gêneros no conjunto de validação\n",
    "df_valid['SRC_GENRE'] = df_valid['SRC_GENRE'].apply(lambda x: x.split('|'))\n",
    "genres_encoded_valid = mlb.transform(df_valid['SRC_GENRE'])\n",
    "\n",
    "# Fazer previsões no conjunto de validação\n",
    "y_pred_probs_valid = model.predict([padded_sequences_valid, df_valid[['PLOT_SUMMARY_SENTIMENT_ENCODED', 'SRC_TITLE_NM_SENTIMENT_ENCODED',\n",
    "       'PLOT_OUTLINE_SENTIMENT_ENCODED', 'PLOT_MEDIUM_SENTIMENT_ENCODED',\n",
    "       'PLOT_SUMMARY_DOMINANT_TOPIC', 'PLOT_OUTLINE_DOMINANT_TOPIC',\n",
    "       'PLOT_MEDIUM_DOMINANT_TOPIC', 'SRC_TITLE_NM_DOMINANT_TOPIC','RATING_AVG']]])\n",
    "y_pred_classes_valid = np.argmax(y_pred_probs_valid, axis=1)\n",
    "predicted_genres_valid = mlb.classes_[y_pred_classes_valid]\n",
    "\n",
    "# Adicionar as previsões como uma coluna ao DataFrame df_valid\n",
    "df_valid['Predicted_Genre'] = predicted_genres_valid\n",
    "\n",
    "# Calcular a acurácia média no conjunto de validação\n",
    "accuracy_valid = accuracy_score(np.argmax(genres_encoded_valid, axis=1), y_pred_classes_valid)\n",
    "\n",
    "# Calcular a acurácia por gênero no conjunto de validação\n",
    "genre_accuracy_valid = {}\n",
    "for i, genre in enumerate(mlb.classes_):\n",
    "    genre_accuracy_valid[genre] = accuracy_score(genres_encoded_valid[:, i], y_pred_probs_valid[:, i] > 0.5)\n",
    "\n",
    "# Exibir resultados\n",
    "print('Overall Accuracy (Validation):', accuracy_valid)\n",
    "print('\\n\\nGenre-wise Accuracy (Validation):\\n')\n",
    "for genre, acc in genre_accuracy_valid.items():\n",
    "    print(f'{genre}: {acc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcef9852",
   "metadata": {},
   "source": [
    "### Predctions for Test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "644f8baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 43s 114ms/step\n"
     ]
    }
   ],
   "source": [
    "df_test['processed_title'] = df_test['SRC_TITLE_NM'].fillna('').apply(preprocess_text)\n",
    "df_test['processed_plot'] = df_test['PLOT_SUMMARY'].fillna('').apply(preprocess_text)\n",
    "df_test['processed_plot_outline'] = df_test['PLOT_OUTLINE'].fillna('').apply(preprocess_text)\n",
    "df_test['processed_plot_medium'] = df_test['PLOT_MEDIUM'].fillna('').apply(preprocess_text)\n",
    "\n",
    "\n",
    "df_test['combined_text'] = df_test['processed_title'] + ' ' + df_test['processed_plot'] + df_test['processed_plot_outline']+df_test['processed_plot_medium'] \n",
    "\n",
    "\n",
    "# Tokenizar e preencher as sequências de texto para os dados de validação\n",
    "sequences_valid = tokenizer.texts_to_sequences(df_test['combined_text'])\n",
    "padded_sequences_valid = pad_sequences(sequences_valid, maxlen=300)\n",
    "\n",
    "\n",
    "# Fazer previsões no conjunto de validação\n",
    "y_pred_probs_valid = model.predict([padded_sequences_valid, df_test[['PLOT_SUMMARY_SENTIMENT_ENCODED', 'SRC_TITLE_NM_SENTIMENT_ENCODED',\n",
    "       'PLOT_OUTLINE_SENTIMENT_ENCODED', 'PLOT_MEDIUM_SENTIMENT_ENCODED',\n",
    "       'PLOT_SUMMARY_DOMINANT_TOPIC', 'PLOT_OUTLINE_DOMINANT_TOPIC',\n",
    "       'PLOT_MEDIUM_DOMINANT_TOPIC', 'SRC_TITLE_NM_DOMINANT_TOPIC','RATING_AVG']]])\n",
    "y_pred_classes_valid = np.argmax(y_pred_probs_valid, axis=1)\n",
    "predicted_genres_valid = mlb.classes_[y_pred_classes_valid]\n",
    "\n",
    "# Adicionar as previsões como uma coluna ao DataFrame df_valid\n",
    "df_test['Predicted_Genre'] = predicted_genres_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "625113b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SRC_TITLE_ID</th>\n",
       "      <th>Predicted_Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68001</td>\n",
       "      <td>tt10691314</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68002</td>\n",
       "      <td>tt13871408</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68003</td>\n",
       "      <td>tt1234430</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68004</td>\n",
       "      <td>tt11737766</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68005</td>\n",
       "      <td>tt4122904</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>79996</td>\n",
       "      <td>tt2073661</td>\n",
       "      <td>Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>79997</td>\n",
       "      <td>tt2712420</td>\n",
       "      <td>Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>79998</td>\n",
       "      <td>tt4044464</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>79999</td>\n",
       "      <td>tt5300268</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>80000</td>\n",
       "      <td>tt26922538</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID SRC_TITLE_ID Predicted_Genre\n",
       "0      68001   tt10691314           Drama\n",
       "1      68002   tt13871408           Drama\n",
       "2      68003    tt1234430           Drama\n",
       "3      68004   tt11737766          Action\n",
       "4      68005    tt4122904          Comedy\n",
       "...      ...          ...             ...\n",
       "11995  79996    tt2073661          Horror\n",
       "11996  79997    tt2712420          Horror\n",
       "11997  79998    tt4044464           Drama\n",
       "11998  79999    tt5300268           Drama\n",
       "11999  80000   tt26922538     Documentary\n",
       "\n",
       "[12000 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[[ 'ID','SRC_TITLE_ID','Predicted_Genre']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f69995d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Drama          3969\n",
       "Documentary    3669\n",
       "Comedy         1813\n",
       "Action         1026\n",
       "Horror          978\n",
       "Crime           282\n",
       "Adventure       263\n",
       "Name: Predicted_Genre, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['Predicted_Genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f0bdee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[[ 'ID','Predicted_Genre']].to_csv('output/LSTM_US_test_set_predctions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
